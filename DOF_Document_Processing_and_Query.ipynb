{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generador de Embeddings para Documentos del DOF\n",
    "\n",
    "## √çndice de Contenidos\n",
    "- [1. Descripci√≥n General](#1)\n",
    "- [2. Caracter√≠sticas Principales](#2)\n",
    "- [3. Requisitos y Dependencias](#3)\n",
    "- [4. Instalaci√≥n](#4)\n",
    "- [5. Estructura de Directorios](#5)\n",
    "- [6. Importaci√≥n de Librer√≠as](#6)\n",
    "- [7. Configuraci√≥n de Modelos](#7)\n",
    "- [8. Inicializaci√≥n de Base de Datos](#8)\n",
    "- [9. Funciones de Procesamiento](#9)\n",
    "- [10. Procesamiento de Archivos](#10)\n",
    "- [11. Widget para Procesamiento](#11)\n",
    "- [12. Procesamiento Manual](#12)\n",
    "- [13. Visor de Chunks](#13)\n",
    "- [14. Sistema de Consulta con Gemini](#14)\n",
    "- [15. Notas Importantes](#15)\n",
    "\n",
    "<h2 id=\"1\">1. Descripci√≥n General</h2>\n",
    "\n",
    "Este sistema procesa archivos markdown del Diario Oficial de la Federaci√≥n (DOF), genera embeddings sem√°nticos y permite realizar consultas utilizando b√∫squeda vectorial e inteligencia artificial. El sistema divide los documentos en secciones jer√°rquicas basadas en sus encabezados, fragmenta el texto y utiliza modelos de embedding para indexar el contenido.\n",
    "\n",
    "El notebook implementa:\n",
    "- Detecci√≥n autom√°tica de encabezados en Markdown\n",
    "- Divisi√≥n del texto en chunks suaves (sin romper oraciones)\n",
    "- Generaci√≥n de encabezados contextuales para cada fragmento\n",
    "- Almacenamiento en base de datos SQLite con capacidades vectoriales\n",
    "- Consultas sem√°nticas con Gemini 2.0\n",
    "\n",
    "<h2 id=\"2\">2. Caracter√≠sticas Principales</h2>\n",
    "\n",
    "- **Procesamiento Jer√°rquico**: Detecta autom√°ticamente encabezados y subt√≠tulos para mantener la estructura del documento.\n",
    "- **Fragmentaci√≥n Suave**: Divide el texto respetando l√≠mites naturales como oraciones.\n",
    "- **Generaci√≥n de Embeddings**: Utiliza modelos modernos para convertir texto a vectores sem√°nticos.\n",
    "- **Almacenamiento Vectorial**: Base de datos SQLite optimizada para b√∫squedas vectoriales.\n",
    "- **Consultas con IA**: Integraci√≥n con Gemini 2.0 para respuestas contextualizadas.\n",
    "- **Interfaz Interactiva**: Widgets interactivos para procesamiento y consultas.\n",
    "\n",
    "<h2 id=\"3\">3. Requisitos y Dependencias</h2>\n",
    "\n",
    "El sistema depende de las siguientes bibliotecas y tecnolog√≠as:\n",
    "\n",
    "- typer: para la interfaz de l√≠nea de comandos\n",
    "- fastlite: para manipulaci√≥n simplificada de bases de datos SQLite\n",
    "- sqlite-vec: para habilitar capacidades vectoriales en SQLite\n",
    "- sentence-transformers: para la generaci√≥n de embeddings\n",
    "- tokenizers: para tokenizar el texto\n",
    "- tqdm: para mostrar barras de progreso\n",
    "- google.generativeai: para integraci√≥n con Gemini 2.0\n",
    "- numpy: para procesamiento num√©rico\n",
    "- python-dotenv: para cargar variables de entorno\n",
    "- ipywidgets: para widgets interactivos\n",
    "\n",
    "<h2 id=\"4\">4. Instalaci√≥n</h2>\n",
    "\n",
    "1. Clona el repositorio o descarga los archivos\n",
    "2. Instala las dependencias necesarias usando uv (instalador de paquetes m√°s r√°pido y moderno):\n",
    "\n",
    "```bash\n",
    "# Instalar uv si no lo tienes\n",
    "curl -sSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Instalar dependencias con uv\n",
    "uv pip install typer fastlite sqlite-vec sentence-transformers tokenizers tqdm google-generativeai numpy python-dotenv ipywidgets\n",
    "```\n",
    "\n",
    "3. Configura una API key de Google Gemini en un archivo .env:\n",
    "\n",
    "```\n",
    "GOOGLE_API_KEY=tu_api_key_aqu√≠\n",
    "```\n",
    "\n",
    "<h2 id=\"5\">5. Estructura de Directorios</h2>\n",
    "\n",
    "```\n",
    "‚îú‚îÄ‚îÄ DOF_Embeddings_Generator.ipynb # Notebook principal\n",
    "‚îú‚îÄ‚îÄ dof_db/ # Directorio para la base de datos\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ db.sqlite # Base de datos SQLite\n",
    "‚îú‚îÄ‚îÄ .env # Archivo de variables de entorno\n",
    "‚îî‚îÄ‚îÄ documentos_dof/ # Documentos markdown para procesar\n",
    "```\n",
    "\n",
    "<h2 id=\"6\">6. Importaci√≥n de Librer√≠as</h2>\n",
    "\n",
    "En esta secci√≥n se importan todas las bibliotecas necesarias para el funcionamiento del sistema. Incluye bibliotecas para:\n",
    "- Manipulaci√≥n de archivos y expresiones regulares\n",
    "- Base de datos y vectores\n",
    "- Modelos de embeddings\n",
    "- Widgets interactivos\n",
    "- Integraci√≥n con Gemini\n",
    "\n",
    "<h2 id=\"7\">7. Configuraci√≥n de Modelos</h2>\n",
    "\n",
    "Configuraci√≥n del modelo de embeddings y tokenizador:\n",
    "- Se utiliza \"nomic-ai/modernbert-embed-base\" para generar embeddings vectoriales de alta calidad\n",
    "- Se establece la longitud m√°xima de cada chunk (por defecto 1000 caracteres)\n",
    "\n",
    "<h2 id=\"8\">8. Inicializaci√≥n de Base de Datos</h2>\n",
    "\n",
    "Esta secci√≥n configura la base de datos SQLite con extensiones vectoriales:\n",
    "- Crea/conecta a la base de datos en \"dof_db/db.sqlite\"\n",
    "- Habilita extensiones para b√∫squeda vectorial con sqlite-vec\n",
    "- Define esquema para tablas \"documents\" y \"chunks\"\n",
    "- Establece relaciones entre documentos y sus fragmentos\n",
    "\n",
    "<h2 id=\"9\">9. Funciones de Procesamiento</h2>\n",
    "\n",
    "Funciones clave para el procesamiento de documentos:\n",
    "\n",
    "1. **parse_text_by_headings**: Divide el texto en secciones basadas en la jerarqu√≠a de encabezados\n",
    "2. **split_text_smooth**: Divide el texto en chunks respetando l√≠mites de oraciones\n",
    "3. **build_chunk_header**: Genera encabezados contextuales para cada fragmento\n",
    "4. **get_url_from_filename**: Obtiene la URL oficial del DOF a partir del nombre del archivo\n",
    "\n",
    "<h2 id=\"10\">10. Procesamiento de Archivos</h2>\n",
    "\n",
    "Estas funciones manejan el procesamiento completo de archivos y directorios:\n",
    "\n",
    "1. **process_file**: Procesa un archivo markdown individual\n",
    "   - Extrae metadatos del documento\n",
    "   - Parsea la estructura jer√°rquica\n",
    "   - Divide el texto en chunks\n",
    "   - Genera embeddings\n",
    "   - Almacena todo en la base de datos\n",
    "   - Crea un archivo de depuraci√≥n\n",
    "\n",
    "2. **process_directory**: Procesa todos los archivos markdown en un directorio\n",
    "\n",
    "<h2 id=\"11\">11. Widget para Procesamiento</h2>\n",
    "\n",
    "Interfaz interactiva para procesar documentos del DOF. Pasos para usarlo:\n",
    "\n",
    "1. Ingresa la ruta al directorio que contiene los archivos markdown del DOF\n",
    "2. Haz clic en \"Procesar archivos\" para iniciar el procesamiento\n",
    "3. Espera a que se complete el proceso y revisa los resultados\n",
    "\n",
    "<h2 id=\"12\">12. Procesamiento Manual (opcional)</h2>\n",
    "\n",
    "Esta secci√≥n permite ejecutar el procesamiento program√°ticamente sin usar los widgets.\n",
    "\n",
    "<h2 id=\"13\">13. Visor de Chunks Generados</h2>\n",
    "\n",
    "Interfaz para visualizar y explorar los chunks generados. Pasos para usarlo:\n",
    "\n",
    "1. Haz clic en \"Buscar archivos\" para encontrar todos los archivos de chunks generados\n",
    "2. Selecciona un archivo del men√∫ desplegable\n",
    "3. Haz clic en \"Ver contenido\" para mostrar los chunks y sus encabezados\n",
    "\n",
    "<h2 id=\"14\">14. Sistema de Consulta con Gemini 2.0</h2>\n",
    "\n",
    "Esta secci√≥n implementa el sistema de b√∫squeda sem√°ntica y consulta con IA. Proceso de consulta:\n",
    "\n",
    "1. Ingresa una pregunta sobre los documentos del DOF\n",
    "2. El sistema busca los chunks m√°s relevantes mediante similitud de embeddings\n",
    "3. Se obtiene contexto extendido (chunks adyacentes) para mejorar la comprensi√≥n\n",
    "4. Se formula un prompt para Gemini 2.0 con el contexto relevante\n",
    "5. Gemini 2.0 genera una respuesta contextualizada\n",
    "6. Se muestra la respuesta junto con informaci√≥n sobre la fuente\n",
    "\n",
    "<h2 id=\"15\">15. Notas Importantes</h2>\n",
    "\n",
    "- Los nombres de archivo deben seguir el formato `DDMMYYYY-XXX.md` para generar URLs correctas.\n",
    "- Para obtener mejores resultados, aseg√∫rate de que los documentos tengan una estructura clara con encabezados markdown (# T√≠tulo, ## Subt√≠tulo, etc.).\n",
    "- La calidad de las respuestas depende de la riqueza del contenido indexado.\n",
    "- El tama√±o m√°ximo de chunk (1000 caracteres) podr√≠a necesitar ajustes seg√∫n el modelo de embeddings.\n",
    "- La visualizaci√≥n de resultados est√° optimizada para notebooks Jupyter.\n",
    "- El rendimiento puede verse afectado cuando la base de datos contiene muchos documentos.\n",
    "\n",
    "[Volver al √çndice](#√çndice-de-Contenidos) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importaci√≥n de librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import typer\n",
    "from fastlite import database\n",
    "from sqlite_vec import load, serialize_float32\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tokenizers import Tokenizer\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "from os import getenv\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Importar widgets para la interfaz interactiva\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuraci√≥n de modelos y tokenizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de modelos y tokenizadores\n",
    "tokenizer = Tokenizer.from_pretrained(\"nomic-ai/modernbert-embed-base\")\n",
    "model = SentenceTransformer(\"nomic-ai/modernbert-embed-base\", trust_remote_code=True)\n",
    "\n",
    "# Configuraci√≥n: m√°ximo de caracteres para cada chunk suave\n",
    "MAX_CHUNK_LENGTH = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializaci√≥n de la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Table chunks (id, document_id, text, header, embedding, created_at)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializar la base de datos con sqlite-vec\n",
    "db = database(\"dof_db/db.sqlite\")\n",
    "db.conn.enable_load_extension(True)\n",
    "load(db.conn)\n",
    "db.conn.enable_load_extension(False)\n",
    "\n",
    "# Crear/actualizar esquema de tablas\n",
    "db.t.documents.create(\n",
    "    id=int, \n",
    "    title=str, \n",
    "    url=str, \n",
    "    file_path=str, \n",
    "    created_at=datetime, \n",
    "    pk=\"id\", \n",
    "    ignore=True\n",
    ")\n",
    "db.t.documents.create_index([\"url\"], unique=True, if_not_exists=True)\n",
    "\n",
    "# Se a√±ade la columna 'header' para guardar el encabezado contextual\n",
    "db.t.chunks.create(\n",
    "    id=int,\n",
    "    document_id=int,\n",
    "    text=str,\n",
    "    header=str,\n",
    "    embedding=bytes,\n",
    "    created_at=datetime,\n",
    "    pk=\"id\",\n",
    "    foreign_keys=[(\"document_id\", \"documents\")],\n",
    "    ignore=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones para procesar el documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex para detectar headings en Markdown\n",
    "HEADING_PATTERN = re.compile(r'^(#{1,6})\\s+(.*)$')\n",
    "\n",
    "def parse_text_by_headings(text: str):\n",
    "    \"\"\"\n",
    "    Divide el texto en secciones basadas en los headings de Markdown.\n",
    "    Retorna una lista de diccionarios con:\n",
    "      - \"heading_hierarchy\": lista con la jerarqu√≠a de headings\n",
    "      - \"content\": contenido de la secci√≥n\n",
    "    \"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    sections = []\n",
    "    current_hierarchy = []\n",
    "    current_content_lines = []\n",
    "\n",
    "    def add_section(hierarchy, content_lines):\n",
    "        if not hierarchy or not content_lines:\n",
    "            return\n",
    "        sections.append({\n",
    "            \"heading_hierarchy\": hierarchy.copy(),\n",
    "            \"content\": \"\\n\".join(content_lines).strip()\n",
    "        })\n",
    "\n",
    "    for line in lines:\n",
    "        heading_match = HEADING_PATTERN.match(line)\n",
    "        if heading_match:\n",
    "            # Cuando se detecta un heading, se cierra la secci√≥n anterior\n",
    "            add_section(current_hierarchy, current_content_lines)\n",
    "            current_content_lines = []\n",
    "            hashes = heading_match.group(1)\n",
    "            heading_text = heading_match.group(2).strip()\n",
    "            level = len(hashes)\n",
    "            # Ajustar la jerarqu√≠a: mantener los niveles anteriores hasta (nivel-1)\n",
    "            current_hierarchy = current_hierarchy[:level-1]\n",
    "            current_hierarchy.append(heading_text)\n",
    "        else:\n",
    "            current_content_lines.append(line)\n",
    "\n",
    "    # Agregar la √∫ltima secci√≥n si existe contenido\n",
    "    add_section(current_hierarchy, current_content_lines)\n",
    "    return sections\n",
    "\n",
    "def split_text_smooth(text: str, max_length: int = MAX_CHUNK_LENGTH, min_chunk_ratio: float = 0.5):\n",
    "    \"\"\"\n",
    "    Divide el texto en chunks suaves sin romper oraciones.\n",
    "    Separa el texto en oraciones bas√°ndose en signos de puntuaci√≥n y las agrupa sin\n",
    "    exceder el l√≠mite de caracteres (max_length). Si el √∫ltimo chunk es muy corto\n",
    "    (menos de min_chunk_ratio * max_length), se fusiona con el chunk anterior.\n",
    "    \"\"\"\n",
    "    # Separa el texto en oraciones (manteniendo el signo de puntuaci√≥n)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) + 1 <= max_length:\n",
    "            current_chunk += sentence + \" \"\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \" \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        current_chunk = current_chunk.strip()\n",
    "        # Si el √∫ltimo chunk es muy corto y existe un chunk previo, fusi√≥nalo\n",
    "        if chunks and len(current_chunk) < max_length * min_chunk_ratio:\n",
    "            previous_chunk = chunks.pop()\n",
    "            merged = previous_chunk + \" \" + current_chunk\n",
    "            chunks.append(merged.strip())\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def build_chunk_header(doc_title: str, heading_hierarchy: list):\n",
    "    \"\"\"\n",
    "    Construye el encabezado contextual utilizando el t√≠tulo del documento y la jerarqu√≠a de headings.\n",
    "    \"\"\"\n",
    "    if not heading_hierarchy:\n",
    "        return f\"Document: {doc_title}\"\n",
    "    hierarchy_str = \" > \".join(heading_hierarchy)\n",
    "    return f\"Document: {doc_title} | Section: {hierarchy_str}\"\n",
    "\n",
    "def get_url_from_filename(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Genera la URL con base en el patr√≥n del nombre de archivo.\n",
    "    Formato esperado: DDMMYYYY-XXX.md\n",
    "    Ejemplo: 23012025-MAT.md\n",
    "    \"\"\"\n",
    "    base_filename = os.path.basename(filename).replace(\".md\", \"\")\n",
    "    if len(base_filename) >= 8:\n",
    "        year = base_filename[4:8]\n",
    "        pdf_filename = f\"{base_filename}.pdf\"\n",
    "        url = f\"https://diariooficial.gob.mx/abrirPDF.php?archivo={pdf_filename}&anio={year}&repo=repositorio/\"\n",
    "        return url\n",
    "    else:\n",
    "        raise ValueError(f\"Expected filename like 23012025-MAT.md but got {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones para procesamiento de archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path: str):\n",
    "    \"\"\"\n",
    "    Procesa un archivo markdown, extrae secciones basadas en headings,\n",
    "    divide cada secci√≥n en chunks suaves, genera embeddings y los almacena en la base de datos.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # 1) Extraer metadatos y preparar la inserci√≥n del documento\n",
    "    title = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    url = get_url_from_filename(file_path)\n",
    "\n",
    "    # Eliminar cualquier versi√≥n anterior del documento\n",
    "    db.t.documents.delete_where(\"url = ?\", [url])\n",
    "    doc = db.t.documents.insert(\n",
    "        title=title, \n",
    "        url=url, \n",
    "        file_path=file_path, \n",
    "        created_at=datetime.now()\n",
    "    )\n",
    "\n",
    "    # 2) Parsear el contenido basado en headings\n",
    "    sections = parse_text_by_headings(content)\n",
    "\n",
    "    # Archivo de salida para depuraci√≥n de chunks\n",
    "    chunks_file_path = os.path.splitext(file_path)[0] + \"_chunks.txt\"\n",
    "    # Definimos un contador global para numerar secuencialmente los chunks\n",
    "    global_chunk_counter = 1\n",
    "    with open(chunks_file_path, \"w\", encoding=\"utf-8\") as chunks_file:\n",
    "        # 3) Procesar cada secci√≥n\n",
    "        for section in sections:\n",
    "            hierarchy = section.get(\"heading_hierarchy\", [])\n",
    "            section_content = section.get(\"content\", \"\")\n",
    "            # Construir el encabezado contextual\n",
    "            header = build_chunk_header(title, hierarchy)\n",
    "            # Dividir la secci√≥n en chunks suaves\n",
    "            sub_chunks = split_text_smooth(section_content, max_length=MAX_CHUNK_LENGTH)\n",
    "\n",
    "            for i, chunk in enumerate(sub_chunks):\n",
    "                # Texto completo para el embedding: encabezado + chunk\n",
    "                text_for_embedding = f\"{header}\\n\\n{chunk}\"\n",
    "                embedding = model.encode(f\"search_document: {text_for_embedding}\")\n",
    "\n",
    "                # Guardar en el archivo de depuraci√≥n\n",
    "                chunks_file.write(f\"--- CHUNK #{global_chunk_counter} ---\\n\")\n",
    "                chunks_file.write(f\"Header: {header}\\n\")\n",
    "                chunks_file.write(f\"Texto:\\n{chunk}\\n\")\n",
    "                chunks_file.write(\"\\n\" + \"-\"*50 + \"\\n\\n\")\n",
    "                global_chunk_counter += 1\n",
    "\n",
    "                # Insertar en la base de datos\n",
    "                db.t.chunks.insert(\n",
    "                    document_id=doc[\"id\"],\n",
    "                    text=chunk,\n",
    "                    header=header,\n",
    "                    embedding=embedding,\n",
    "                    created_at=datetime.now(),\n",
    "                )\n",
    "\n",
    "    return f\"‚úÖ Procesado completado para: {file_path}\\nSe gener√≥ el archivo de chunks en: {chunks_file_path}\"\n",
    "\n",
    "def process_directory(directory: str):\n",
    "    \"\"\"Procesa todos los archivos .md de un directorio.\"\"\"\n",
    "    results = []\n",
    "    md_files = [f for f in os.listdir(directory) if f.endswith(\".md\")]\n",
    "    \n",
    "    if not md_files:\n",
    "        return \"‚ö†Ô∏è No se encontraron archivos .md en el directorio especificado.\"\n",
    "    \n",
    "    for f in tqdm(md_files, desc=\"Procesando archivos\"):\n",
    "        file_path = os.path.join(directory, f)\n",
    "        result = process_file(file_path)\n",
    "        results.append(result)\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widget para ingresar la ruta de procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b96112319244b548a2f1ec51ccfb04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3>Procesamiento de archivos DOF</h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d15aef305b4fb189ee4310ecaac837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Directorio:', layout=Layout(width='80%'), placeholder='Ingresa la ruta del directo‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef234ca8c80341408a31b47c3e399f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Procesar archivos', icon='check', style=ButtonStyle(), tooltip='Ha‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b11b105be644d7bb98118b7acffc650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear widget de entrada para la ruta del directorio\n",
    "text_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Ingresa la ruta del directorio con archivos .md',\n",
    "    description='Directorio:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "process_button = widgets.Button(\n",
    "    description='Procesar archivos',\n",
    "    button_style='primary',\n",
    "    tooltip='Haz clic para procesar los archivos en el directorio especificado',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        directory = text_input.value.strip()\n",
    "    md_files = [f for f in os.listdir(directory) if f.endswith(\".md\")]\n",
    "    \n",
    "    if not md_files:\n",
    "        return \"‚ö†Ô∏è No se encontraron archivos .md en el directorio especificado.\"\n",
    "    \n",
    "    for f in tqdm(md_files, desc=\"Procesando archivos\"):\n",
    "        file_path = os.path.join(directory, f)\n",
    "        result = process_file(file_path)\n",
    "        results.append(result)\n",
    "    \n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        directory = text_input.value.strip()\n",
    "        \n",
    "        if not directory:\n",
    "            print(\"‚ö†Ô∏è Por favor ingresa una ruta de directorio v√°lida.\")\n",
    "            return\n",
    "        \n",
    "        if not os.path.exists(directory):\n",
    "            print(f\"‚ö†Ô∏è El directorio '{directory}' no existe.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"üîÑ Procesando archivos en: {directory}\")\n",
    "        try:\n",
    "            result = process_directory(directory)\n",
    "            print(result)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error durante el procesamiento: {str(e)}\")\n",
    "\n",
    "process_button.on_click(on_button_clicked)\n",
    "\n",
    "# Mostrar los widgets\n",
    "display(widgets.HTML(\"<h3>Procesamiento de archivos DOF</h3>\"))\n",
    "display(text_input)\n",
    "display(process_button)\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de procesamiento manual (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de c√≥mo procesar un directorio espec√≠fico manualmente\n",
    "# Descomenta la siguiente l√≠nea y especifica la ruta\n",
    "# result = process_directory(\"ruta/a/tu/directorio\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visor de Chunks Generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30889b364c014386bcf4990050efeff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3>Visor de Chunks Generados</h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829fc801649d400fbe5e203dd2d46bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='info', description='Buscar archivos', icon='refresh', style=ButtonStyle(),‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6232e21051ef4e969ed5138d5e1af794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Ver contenido', icon='eye', style=ButtonStyle(), tooltip='Mostrar ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed46f29dd1b4b5894ae521fa7d41247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid #ddd', border_left='1px solid #ddd', border_right='1px solid #dd‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import glob\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Widget para seleccionar archivo de chunks\n",
    "file_selector = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description='Archivo:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "refresh_button = widgets.Button(\n",
    "    description='Buscar archivos',\n",
    "    button_style='info',\n",
    "    tooltip='Actualizar lista de archivos de chunks',\n",
    "    icon='refresh'\n",
    ")\n",
    "\n",
    "view_button = widgets.Button(\n",
    "    description='Ver contenido',\n",
    "    button_style='success',\n",
    "    tooltip='Mostrar el contenido del archivo seleccionado',\n",
    "    icon='eye'\n",
    ")\n",
    "\n",
    "chunk_output = widgets.Output(layout=widgets.Layout(\n",
    "    border='1px solid #ddd',\n",
    "    max_height='500px',\n",
    "    overflow_y='auto',\n",
    "    padding='10px'\n",
    "))\n",
    "\n",
    "def update_file_list(b=None):\n",
    "    # Buscar todos los archivos _chunks.txt en el directorio de la entrada de texto\n",
    "    directory = text_input.value.strip() if hasattr(text_input, 'value') and text_input.value.strip() else '.'\n",
    "    chunk_files = glob.glob(os.path.join(directory, \"*_chunks.txt\"))\n",
    "    \n",
    "    # A√±adir tambi√©n la b√∫squeda recursiva en subdirectorios\n",
    "    if not chunk_files:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith(\"_chunks.txt\"):\n",
    "                    chunk_files.append(os.path.join(root, file))\n",
    "    \n",
    "    # Actualizar opciones del selector\n",
    "    file_selector.options = [os.path.basename(f) for f in sorted(chunk_files)]\n",
    "    file_selector.paths = {os.path.basename(f): f for f in sorted(chunk_files)}\n",
    "    \n",
    "    with chunk_output:\n",
    "        chunk_output.clear_output()\n",
    "        if not chunk_files:\n",
    "            print(f\"‚ùå No se encontraron archivos de chunks en '{directory}' ni sus subdirectorios.\")\n",
    "        else:\n",
    "            print(f\"‚úÖ Se encontraron {len(chunk_files)} archivos de chunks.\")\n",
    "\n",
    "def display_chunk_file(b):\n",
    "    with chunk_output:\n",
    "        chunk_output.clear_output()\n",
    "        if not file_selector.options:\n",
    "            print(\"‚ùå No hay archivos para mostrar. Usa 'Buscar archivos' primero.\")\n",
    "            return\n",
    "            \n",
    "        selected_file = file_selector.value\n",
    "        full_path = file_selector.paths[selected_file]\n",
    "        \n",
    "        try:\n",
    "            with open(full_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Formatear el contenido para HTML\n",
    "            content_html = content.replace('\\n', '<br>')\n",
    "            content_html = re.sub(r'--- CHUNK #(\\d+) ---', r'<h4 style=\"color:blue;\">--- CHUNK #\\1 ---</h4>', content_html)\n",
    "            content_html = re.sub(r'Header: (.*?)<br>', r'<b>Header:</b> <span style=\"color:green;\">\\1</span><br>', content_html)\n",
    "            content_html = re.sub(r'-{50}', r'<hr style=\"border-top: 1px dashed #ccc;\">', content_html)\n",
    "            \n",
    "            display(HTML(f\"<h3>Contenido de: {selected_file}</h3>\"))\n",
    "            display(HTML(content_html))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error al leer el archivo: {str(e)}\")\n",
    "\n",
    "refresh_button.on_click(update_file_list)\n",
    "view_button.on_click(display_chunk_file)\n",
    "\n",
    "# Mostrar widgets\n",
    "display(widgets.HTML(\"<h3>Visor de Chunks Generados</h3>\"))\n",
    "display(widgets.HBox([refresh_button, file_selector]))\n",
    "display(view_button)\n",
    "display(chunk_output)\n",
    "\n",
    "# Inicializar la lista de archivos\n",
    "update_file_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sistema de Consulta con Gemini 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99b71724857479eb3b08e2a6c0f37c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3>Consulta de Documentos DOF con Gemini 2.0</h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6813b1e0e03e4c058dc0b302488335cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Ingresa una consulta sobre los documentos del Diario Oficial de la Federaci√≥n:'), ‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar variables de entorno y configurar Gemini\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"‚ö†Ô∏è GOOGLE_API_KEY no est√° configurada en las variables de entorno\")\n",
    "else:\n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "# Usar el mismo modelo de embeddings que usamos para indexar\n",
    "model_query = SentenceTransformer(\"nomic-ai/modernbert-embed-base\", trust_remote_code=True)\n",
    "\n",
    "# Funciones para procesamiento de consultas\n",
    "\n",
    "def deserialize_embedding(blob):\n",
    "    \"\"\"Convierte un BLOB almacenado en la base de datos a un vector NumPy de tipo float32.\"\"\"\n",
    "    return np.frombuffer(blob, dtype=np.float32)\n",
    "\n",
    "def get_all_chunks():\n",
    "    \"\"\"Obtiene todos los chunks almacenados en la base de datos.\"\"\"\n",
    "    results = db.query(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            c.id as chunk_id,\n",
    "            c.text as chunk_text,\n",
    "            c.header as chunk_header,\n",
    "            c.document_id as document_id,\n",
    "            d.title as doc_title,\n",
    "            d.url as doc_url,\n",
    "            c.embedding as embedding_blob\n",
    "        FROM chunks c\n",
    "        JOIN documents d ON c.document_id = d.id\n",
    "        ORDER BY c.id;\n",
    "        \"\"\"\n",
    "    )\n",
    "    return list(results)\n",
    "\n",
    "def find_relevant_chunks(query, all_chunks, top_k=10):\n",
    "    \"\"\"Encuentra los chunks m√°s relevantes para la consulta.\"\"\"\n",
    "    # Generar embedding para la consulta\n",
    "    query_embedding = model_query.encode(f\"search_query: {query}\")\n",
    "    \n",
    "    # Calcular distancias\n",
    "    scored_results = []\n",
    "    for chunk in all_chunks:\n",
    "        chunk_embedding = deserialize_embedding(chunk[\"embedding_blob\"])\n",
    "        distance = np.linalg.norm(query_embedding - chunk_embedding)\n",
    "        scored_results.append({\n",
    "            \"chunk_id\": chunk[\"chunk_id\"],\n",
    "            \"chunk_text\": chunk[\"chunk_text\"],\n",
    "            \"chunk_header\": chunk[\"chunk_header\"],\n",
    "            \"document_id\": chunk[\"document_id\"],\n",
    "            \"doc_title\": chunk[\"doc_title\"],\n",
    "            \"doc_url\": chunk[\"doc_url\"],\n",
    "            \"distance\": distance,\n",
    "            \"similarity\": 1.0 / (1.0 + distance)  # Convertir distancia a similitud\n",
    "        })\n",
    "    \n",
    "    # Ordenar por menor distancia (mayor similitud)\n",
    "    return sorted(scored_results, key=lambda x: x[\"distance\"])[:top_k]\n",
    "\n",
    "def get_extended_context(best_chunk, all_chunks):\n",
    "    \"\"\"Obtiene un contexto extendido incluyendo chunks adyacentes.\"\"\"\n",
    "    # Filtrar chunks del mismo documento\n",
    "    same_doc_chunks = [c for c in all_chunks if c[\"document_id\"] == best_chunk[\"document_id\"]]\n",
    "    \n",
    "    # Ordenar por ID para mantener el orden original\n",
    "    same_doc_chunks = sorted(same_doc_chunks, key=lambda c: c[\"chunk_id\"])\n",
    "    \n",
    "    # Encontrar la posici√≥n del mejor chunk\n",
    "    best_index = None\n",
    "    for i, chunk in enumerate(same_doc_chunks):\n",
    "        if chunk[\"chunk_id\"] == best_chunk[\"chunk_id\"]:\n",
    "            best_index = i\n",
    "            break\n",
    "    \n",
    "    # Construir contexto extendido\n",
    "    context_parts = []\n",
    "    context_headers = []\n",
    "    \n",
    "    if best_index is not None:\n",
    "        # A√±adir chunk anterior si existe\n",
    "        if best_index > 0:\n",
    "            context_parts.append(same_doc_chunks[best_index - 1][\"chunk_text\"])\n",
    "            context_headers.append(same_doc_chunks[best_index - 1][\"chunk_header\"])\n",
    "            \n",
    "        # A√±adir el mejor chunk\n",
    "        context_parts.append(best_chunk[\"chunk_text\"])\n",
    "        context_headers.append(best_chunk[\"chunk_header\"])\n",
    "        \n",
    "        # A√±adir chunk siguiente si existe\n",
    "        if best_index < len(same_doc_chunks) - 1:\n",
    "            context_parts.append(same_doc_chunks[best_index + 1][\"chunk_text\"])\n",
    "            context_headers.append(same_doc_chunks[best_index + 1][\"chunk_header\"])\n",
    "    else:\n",
    "        # Si no se encuentra (caso raro), usar solo el mejor chunk\n",
    "        context_parts.append(best_chunk[\"chunk_text\"])\n",
    "        context_headers.append(best_chunk[\"chunk_header\"])\n",
    "    \n",
    "    # Generar contexto con headers\n",
    "    full_context = \"\"\n",
    "    for i, (header, text) in enumerate(zip(context_headers, context_parts)):\n",
    "        full_context += f\"[Secci√≥n {i+1}]: {header}\\n\\n{text}\\n\\n\"\n",
    "    \n",
    "    return full_context\n",
    "\n",
    "def query_gemini(query, query_output):\n",
    "    \"\"\"Realiza una consulta completa: b√∫squeda de chunks y generaci√≥n de respuesta con Gemini.\"\"\"\n",
    "    try:\n",
    "        with query_output:\n",
    "            clear_output()\n",
    "            print(f\"üîç Procesando consulta: '{query}'\")\n",
    "            print(\"Obteniendo chunks de la base de datos...\")\n",
    "        \n",
    "            # Obtener todos los chunks\n",
    "            all_chunks = get_all_chunks()\n",
    "            \n",
    "            if not all_chunks:\n",
    "                print(\"‚ùå No se encontraron documentos en la base de datos.\")\n",
    "                return\n",
    "                \n",
    "            print(f\"Se obtuvieron {len(all_chunks)} chunks totales.\")\n",
    "            print(\"Buscando chunks relevantes...\")\n",
    "            \n",
    "            # Encontrar chunks relevantes\n",
    "            top_results = find_relevant_chunks(query, all_chunks)\n",
    "            \n",
    "            if not top_results:\n",
    "                print(\"‚ùå No se encontraron chunks relevantes para esta consulta.\")\n",
    "                return\n",
    "                \n",
    "            # Obtener el mejor resultado\n",
    "            best_chunk = top_results[0]\n",
    "            \n",
    "            print(f\"Se encontraron {len(top_results)} chunks relevantes.\")\n",
    "            print(f\"Mejor chunk: ID={best_chunk['chunk_id']}, Distancia={best_chunk['distance']:.4f}\")\n",
    "            print(\"Obteniendo contexto extendido...\")\n",
    "            \n",
    "            # Obtener contexto extendido (con chunks adyacentes)\n",
    "            extended_context = get_extended_context(best_chunk, all_chunks)\n",
    "            \n",
    "            print(\"Generando prompt para Gemini...\")\n",
    "            \n",
    "            # Construir prompt para Gemini\n",
    "            prompt = (\n",
    "                f\"Utilizando el siguiente contexto extra√≠do de un documento del Diario Oficial de la Federaci√≥n:\\n\\n\"\n",
    "                f\"---\\n{extended_context}\\n---\\n\\n\"\n",
    "                f\"Instrucciones:\\n\"\n",
    "                f\"- Si la consulta est√° relacionada directamente con el contenido, responde bas√°ndote en la informaci√≥n proporcionada.\\n\"\n",
    "                f\"- Si la consulta es un saludo, despedida o una interacci√≥n social com√∫n, responde de forma cordial y adecuada.\\n\"\n",
    "                f\"- Si se solicita informaci√≥n que no se encuentra en el contexto, indica de manera clara que no hay informaci√≥n relevante disponible en el documento.\\n\"\n",
    "                f\"- Aseg√∫rate de interpretar correctamente la solicitud, contestando √∫nicamente lo que se pregunta y evitando agregar informaci√≥n irrelevante.\\n\\n\"\n",
    "                f\"Pregunta:\\n{query}\"\n",
    "            )\n",
    "            \n",
    "            print(\"Enviando consulta a Gemini...\")\n",
    "            \n",
    "            # Generar respuesta con Gemini\n",
    "            model_gemini = genai.GenerativeModel('gemini-2.0-flash')\n",
    "            response = model_gemini.generate_content(prompt)\n",
    "            \n",
    "            # Mostrar respuesta\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(f\"RESPUESTA A: '{query}'\")\n",
    "            print(\"=\"*60)\n",
    "            display(HTML(f\"<div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin: 10px 0;'>{response.text.replace(chr(10), '<br>')}</div>\"))\n",
    "            \n",
    "            print(\"\\n\" + \"-\"*60)\n",
    "            print(f\"Fuente: {best_chunk['doc_title']}\")\n",
    "            print(f\"URL: {best_chunk['doc_url']}\")\n",
    "            print(f\"Relevancia (distancia): {best_chunk['distance']:.4f}\")\n",
    "            print(f\"Similitud: {best_chunk['similarity']:.4f}\")\n",
    "            print(\"-\"*60)\n",
    "            \n",
    "            # Mostrar contexto utilizado en un formato colapsable\n",
    "            context_html = extended_context.replace('\\n', '<br>').replace('[Secci√≥n', '<b>[Secci√≥n')\n",
    "            context_html = context_html.replace(']:', ']:</b>')\n",
    "            \n",
    "            display(HTML(f\"\"\"\n",
    "            <details>\n",
    "                <summary style=\"cursor: pointer; color: #0066cc;\">Ver contexto utilizado (click para expandir)</summary>\n",
    "                <div style=\"background-color: #f5f5f5; padding: 10px; border: 1px solid #ddd; margin-top: 10px;\">\n",
    "                    {context_html}\n",
    "                </div>\n",
    "            </details>\n",
    "            \"\"\"))\n",
    "            \n",
    "    except Exception as e:\n",
    "        with query_output:\n",
    "            print(f\"‚ùå Error durante el proceso: {str(e)}\")\n",
    "\n",
    "# Crear widgets para la interfaz de consulta\n",
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Ingresa tu consulta sobre documentos del DOF',\n",
    "    description='Consulta:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "query_button = widgets.Button(\n",
    "    description='Consultar',\n",
    "    button_style='primary',\n",
    "    tooltip='Enviar consulta a Gemini',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "query_output = widgets.Output(\n",
    "    layout=widgets.Layout(\n",
    "        border='1px solid #ddd',\n",
    "        padding='10px',\n",
    "        overflow_y='auto',\n",
    "        max_height='600px'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Funci√≥n para manejar el evento del bot√≥n\n",
    "def on_query_button_clicked(b):\n",
    "    query = query_input.value.strip()\n",
    "    if not query:\n",
    "        with query_output:\n",
    "            clear_output()\n",
    "            print(\"‚ö†Ô∏è Por favor ingresa una consulta v√°lida.\")\n",
    "        return\n",
    "    \n",
    "    query_gemini(query, query_output)\n",
    "\n",
    "# Funci√≥n para manejar el evento de tecla Enter\n",
    "def on_query_input_submit(sender):\n",
    "    query = query_input.value.strip()\n",
    "    if not query:\n",
    "        with query_output:\n",
    "            clear_output()\n",
    "            print(\"‚ö†Ô∏è Por favor ingresa una consulta v√°lida.\")\n",
    "        return\n",
    "    \n",
    "    query_gemini(query, query_output)\n",
    "\n",
    "def on_query_input_changed(change):\n",
    "    if change.new and not change.old:  # Solo se activa cuando cambia de vac√≠o a un valor\n",
    "        query = change.new.strip()\n",
    "        if query:\n",
    "            query_gemini(query, query_output)\n",
    "\n",
    "query_button.on_click(on_query_button_clicked)\n",
    "query_input.continuous_update = False  \n",
    "# Observar cambios en el valor en lugar de usar on_submit\n",
    "query_input.observe(lambda change: \n",
    "    on_query_gemini_if_enter(change, query_output), \n",
    "    names='value')\n",
    "\n",
    "def on_query_gemini_if_enter(change, output):\n",
    "    # Comprobar si se presion√≥ Enter (el cambio viene con un valor nuevo)\n",
    "    if hasattr(change, 'new') and change.new.strip():\n",
    "        query = change.new.strip()\n",
    "        query_gemini(query, output)\n",
    "\n",
    "# Mostrar la interfaz\n",
    "display(widgets.HTML(\"<h3>Consulta de Documentos DOF con Gemini 2.0</h3>\"))\n",
    "display(widgets.VBox([\n",
    "    widgets.Label(\"Ingresa una consulta sobre los documentos del Diario Oficial de la Federaci√≥n:\"),\n",
    "    widgets.HBox([query_input, query_button]),\n",
    "    query_output\n",
    "]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
